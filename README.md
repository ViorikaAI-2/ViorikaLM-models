# ViorikaLM-models
**An open-source family of Transformer-based language models, trained from scratch on consumer hardware.**

[![Hugging Face Hub](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Models-yellow.svg)](https://huggingface.co/ViorikaAI)
![Status](https://img.shields.io/badge/Status-Under--Training-orange.svg)

## ğŸ§  About the Project

ViorikaLM is a personal research project focused on demonstrating that meaningful language model development is possible without access to industrial-scale compute resources. All models are implemented and pre-trained from the ground up on a **single NVIDIA GTX 1070** GPU.

**Key Goals:**
- ğŸ§ª **Experiment** with Transformer architectures and training methodologies.
- ğŸ¤– **Democratize AI** by proving that model training is accessible on consumer-grade hardware.
- ğŸ“š **Educate** by sharing pre-trained checkpoints and documenting the process.

## ğŸ—‚ï¸ Model Zoo

All models are available on the Hugging Face Hub:

| Model Name | Parameters | Architecture | Status | HF Link |
| :--- | :--- | :--- | :--- | :--- |
| **ViorikaLM-CHAT** | ~200M | GPT-style (12L, 12H) | ğŸš§ Early Pre-Training | [Link](https://huggingface.co/ViorikaAI/ViorikaLM-CHAT) |
| *More coming soon...* | | | | |

## âš™ï¸ Technical Details

- **Framework:** PyTorch
- **Training Hardware:** 1x NVIDIA GTX 1070 (8GB VRAM)
- **Base Architecture:** Decoder-only Transformer (GPT)
- **Pre-training Data:** WikiText-103
- **License:** Not specified (All rights reserved)

## ğŸ“Š Training Progress

Current training status across all models:

| Model | Epoch | Train Loss | Val Loss | Notes |
| :--- | :--- | :--- | :--- | :--- |
| ViorikaLM-CHAT | 2 | ~6.0 | No | Initial pre-training phase |

## ğŸ¤ Contributing & Feedback

This is primarily a research and educational project. We welcome:

- ğŸ’¡ Suggestions and ideas
- ğŸ› Bug reports
- ğŸ“Š Performance feedback
- â­ Starring the repository if you find this project interesting

## ğŸ“œ License

All rights reserved. No license specified.

## ğŸš€ Usage

All models are available via the `transformers` library:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "ViorikaAI/ViorikaLM-CHAT"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)
